{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgzYWh-ef-Jx"
      },
      "source": [
        "Import Library\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ6Nabo9g7Zk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49p1S81jgjYg"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBcJDiHH3KEu"
      },
      "outputs": [],
      "source": [
        "lp_mapping = pd.read_excel('LP and Course Mapping.xlsx', sheet_name=None)\n",
        "resource_data = pd.read_excel('Resource Data Learning Buddy.xlsx', sheet_name=None)\n",
        "\n",
        "datasets = {}\n",
        "datasets.update(lp_mapping)\n",
        "datasets.update(resource_data)\n",
        "\n",
        "print(\"LP and Course Mapping:\", lp_mapping.keys())\n",
        "print(\"Resource Data Learning Buddy:\", resource_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-MYEU0Agyrd"
      },
      "outputs": [],
      "source": [
        "# Pisahkan sheet ke variabel masing-masing\n",
        "# LP and Course Mapping\n",
        "lp_course = lp_mapping['LP + Course']\n",
        "learning_path = lp_mapping['Learning Path']\n",
        "course = lp_mapping['Course']\n",
        "course_level = lp_mapping['Course Level']\n",
        "tutorials = lp_mapping['Tutorials']\n",
        "\n",
        "# Resource Data Learning Buddy\n",
        "lp_answer = resource_data['Learning Path Answer']\n",
        "interest_qs = resource_data['Current Interest Questions']\n",
        "tech_qs = resource_data['Current Tech Questions']\n",
        "skill_keywords = resource_data['Skill Keywords']\n",
        "stud_progress = resource_data['Student Progress']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFJf_P2Hm6vH"
      },
      "outputs": [],
      "source": [
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    display(df.head())\n",
        "    print(f\"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6HSUwjGm4UU"
      },
      "outputs": [],
      "source": [
        "# Dictionary untuk looping\n",
        "datasets = {\n",
        "    \"LP + Course\": lp_course,\n",
        "    \"Learning Path\": learning_path,\n",
        "    \"Course\": course,\n",
        "    \"Course Level\": course_level,\n",
        "    \"Tutorials\": tutorials,\n",
        "    \"Learning Path Answer\": lp_answer,\n",
        "    \"Current Interest Questions\": interest_qs,\n",
        "    \"Current Tech Questions\": tech_qs,\n",
        "    \"Skill Keywords\": skill_keywords,\n",
        "    \"Student Progress\": stud_progress\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv0nv3WHm_VS"
      },
      "outputs": [],
      "source": [
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eenpcRRm9SO"
      },
      "outputs": [],
      "source": [
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    display(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk5dxlxLehP4"
      },
      "source": [
        "Prepocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84GceHzfnnNT"
      },
      "outputs": [],
      "source": [
        "# Cek Missing Value\n",
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    display(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv0pSMHf5hda"
      },
      "outputs": [],
      "source": [
        "# Mengisi NaN dengan 0\n",
        "skill_keywords.fillna(0, inplace=True)\n",
        "stud_progress.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuNTtxF7nxfJ"
      },
      "outputs": [],
      "source": [
        "# Menghapus data duplikat LP + Course\n",
        "lp_course = lp_course.drop_duplicates()\n",
        "print(\"LP + Course duplicates removed:\", lp_course.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SDMOmCE8-Cr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6ggVdd3R3p"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "    text = str(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWDl4tCd8A2J"
      },
      "outputs": [],
      "source": [
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].progress_apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "stopwords = {\n",
        "    \"dan\",\"saya\",\"yang\",\"dengan\",\"ke\",\"di\",\"pada\",\"untuk\",\n",
        "    \"adalah\",\"ini\",\"basic\",\"advanced\",\"intro\",\"beginner\",\n",
        "    \"dasar\",\"program\",\"programming\",\"pemrograman\",\"bahasa\",\n",
        "    \"membuat\",\"make\",\"using\",\"menggunakan\", \"engineer\", \"juggling\", \"jarngan\", \"webflow\", \"WebPageMaker\", \"SOAP API\"\n",
        "    \"Chai\"\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "    # Pastikan text adalah string\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[^\\w\\s/]', ' ', text)\n",
        "    words = [w for w in text.split() if w.lower() not in stopwords]\n",
        "    return \" \".join(words)\n",
        "\n",
        "skill_keywords['keyword'] = skill_keywords['keyword'].fillna(\"\").apply(clean_text)"
      ],
      "metadata": {
        "id": "fIkmev0FnIGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rd7Wsvd_4Gd"
      },
      "outputs": [],
      "source": [
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}\")\n",
        "    display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVWlb8kq71Yk"
      },
      "source": [
        "## Personalized On-boarding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-layer skil assessment"
      ],
      "metadata": {
        "id": "HLQeJOIZ86FH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MANWKd8-9XOU"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "from difflib import get_close_matches"
      ],
      "metadata": {
        "id": "xXUVjDNEB76q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0SoY_xANCOs"
      },
      "outputs": [],
      "source": [
        "# Setup Gemini API\n",
        "genai.configure(api_key=\"AIzaSyBxqU2a_iCkwiHbw785Tl0R7TjUCQeOgr8\")\n",
        "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
        "\n",
        "# Detect Job Role\n",
        "job_role = learning_path['learning_path_name'].dropna().unique().tolist()\n",
        "\n",
        "def detect_job_role(user_input, job_role):\n",
        "    prompt = f\"\"\"\n",
        "    Anda adalah asisten pintar yang bertugas untuk mengklasifikasikan deskripsi pengguna ke job role yang paling tepat.\n",
        "\n",
        "    Petunjuk:\n",
        "    1. Analisis deskripsi pengguna secara menyeluruh.\n",
        "    2. Bandingkan dengan daftar job role berikut, pilih yang paling relevan.\n",
        "    3. Hanya pilih **satu job role** yang paling cocok.\n",
        "    4. Jawaban **hanya berupa nama job role**, tanpa penjelasan, tanda kutip, atau teks tambahan.\n",
        "    5. Jika tidak yakin, pilih job role yang paling mendekati, jangan buat job role baru.\n",
        "\n",
        "    Job role valid:\n",
        "    {job_role}\n",
        "\n",
        "    Deskripsi pengguna:\n",
        "    \"{user_input}\"\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip() if response.text else \"Unknown\"\n",
        "\n",
        "# Detect Skills\n",
        "def detect_skills(user_input, job_role, top_k=6):\n",
        "    prompt = f\"\"\"\n",
        "    Anda adalah AI pakar dalam technical skill assessment untuk berbagai job role IT.\n",
        "\n",
        "    ATURAN SUPER KETAT:\n",
        "    1. Hanya ambil skill dari dataset berikut: {skill_keywords}.\n",
        "    2. Skill harus sesuai job role (bahasa pemrograman, framework, library, library/framework cloud, library/framework yang relevan dengan AI/ML.)\n",
        "    3. DILARANG: skill generik, teori, platform software/cloud, metodologi, tools non-teknis/desain, version control, platform dan library yang tidak relevan dengan job role.\n",
        "    4. Untuk job role Gen AI Engineer menggunakan skill yang sama seperti AI Engineer.\n",
        "    5. Output HARUS EXACT {top_k} skill, pisah koma, tanpa penjelasan.\n",
        "    Job Role: {job_role}\n",
        "    User Input: {user_input}\n",
        "\n",
        "    Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = model.generate_content(prompt)\n",
        "        raw_items = [s.strip() for s in resp.text.replace(\"\\n\", \"\").split(\",\") if s.strip()]\n",
        "    except Exception as e:\n",
        "        print(\"LLM gagal:\", e)\n",
        "        raw_items = []\n",
        "\n",
        "    # Ambil skill yang valid dari dataset saja\n",
        "    valid_keywords = skill_keywords['keyword'].dropna().tolist()\n",
        "    filtered = []\n",
        "\n",
        "    for item in raw_items:\n",
        "        match = get_close_matches(item, valid_keywords, n=1, cutoff=0.6)\n",
        "        if match and match[0] not in filtered:\n",
        "            filtered.append(match[0])\n",
        "\n",
        "    # Jika kurang dari top_k, isi dengan skill lain dari dataset\n",
        "    if len(filtered) < top_k:\n",
        "        remaining = [k for k in valid_keywords if k not in filtered]\n",
        "        filtered.extend(remaining[:top_k - len(filtered)])\n",
        "\n",
        "    return filtered[:top_k]\n",
        "\n",
        "\n",
        "# Ambil pertanyaan dari dataset tech_qs\n",
        "from difflib import SequenceMatcher\n",
        "import pandas as pd\n",
        "\n",
        "def get_questions_for_skill_from_dataset(skill, tech_qs, num_questions=3):\n",
        "    filtered = tech_qs[tech_qs['question_desc'].str.contains(skill, case=False, na=False)]\n",
        "\n",
        "    if len(filtered) >= num_questions:\n",
        "        filtered = filtered.sample(n=num_questions)\n",
        "    elif len(filtered) == 0:\n",
        "        return None\n",
        "\n",
        "    questions = []\n",
        "    for _, row in filtered.iterrows():\n",
        "        # Ambil semua opsi\n",
        "        options = [\n",
        "            str(row['option_1']).strip(),\n",
        "            str(row['option_2']).strip(),\n",
        "            str(row['option_3']).strip(),\n",
        "            str(row['option_4']).strip()\n",
        "        ]\n",
        "\n",
        "        # Format opsi dengan huruf (A. B. C. D.)\n",
        "        formatted_options = [f\"{chr(65+i)}. {opt}\" for i, opt in enumerate(options)]\n",
        "\n",
        "        # Convert correct_answer (text) ke huruf (A/B/C/D)\n",
        "        if pd.isna(row['correct_answer']):\n",
        "            answer_letter = None  # Tandai tidak ada jawaban\n",
        "        else:\n",
        "            correct_text = str(row['correct_answer']).strip()\n",
        "            answer_letter = None  # Default\n",
        "\n",
        "            # Cari exact match dulu (case-insensitive)\n",
        "            for i, opt in enumerate(options):\n",
        "                if opt.lower() == correct_text.lower():\n",
        "                    answer_letter = chr(65 + i)  # 0â†’A, 1â†’B, 2â†’C, 3â†’D\n",
        "                    break\n",
        "\n",
        "            # Jika tidak ketemu, coba fuzzy matching\n",
        "            if answer_letter is None:\n",
        "                best_match_idx = 0\n",
        "                best_similarity = 0\n",
        "\n",
        "                for i, opt in enumerate(options):\n",
        "                    similarity = SequenceMatcher(None, opt.lower(), correct_text.lower()).ratio()\n",
        "                    if similarity > best_similarity:\n",
        "                        best_similarity = similarity\n",
        "                        best_match_idx = i\n",
        "\n",
        "                # Jika similarity > 80%, anggap match\n",
        "                if best_similarity > 0.8:\n",
        "                    answer_letter = chr(65 + best_match_idx)\n",
        "                else:\n",
        "                    print(f\"Warning: Jawaban '{correct_text[:40]}...' tidak ditemukan!\")\n",
        "                    answer_letter = None\n",
        "\n",
        "        questions.append({\n",
        "            \"question\": row['question_desc'],\n",
        "            \"options\": formatted_options,\n",
        "            \"answer\": answer_letter\n",
        "        })\n",
        "\n",
        "    return questions\n",
        "\n",
        "\n",
        "# Generate pertanyaan via Gemini\n",
        "def generate_questions_gemini(skill, num_questions=3):\n",
        "    prompt = f\"\"\"\n",
        "    Anda adalah AI pakar pembuat pertanyaan teknologi.\n",
        "\n",
        "    Buat {num_questions} pertanyaan pilihan ganda terkait skill: \"{skill}\".\n",
        "    Setiap pertanyaan harus memiliki 4 opsi jawaban (A, B, C, D), **tanpa menuliskan jawaban benar**.\n",
        "    Output cukup dalam text biasa, misal:\n",
        "\n",
        "    1. Pertanyaan?\n",
        "    A. jawaban1\n",
        "    B. jawaban2\n",
        "    C. jawaban3\n",
        "    D. jawaban4\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = model.generate_content(prompt)\n",
        "        resp_text = resp.text.strip()\n",
        "\n",
        "        questions = []\n",
        "        q = {}\n",
        "        lines = resp_text.split(\"\\n\")\n",
        "        answer_index_cycle = list(range(4))\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if re.match(r\"^\\d+\\.\", line):\n",
        "                question_text = re.sub(r\"^\\d+\\.\\s*\", \"\", line).strip()\n",
        "                if q:\n",
        "                    if q.get(\"options\"):\n",
        "                        answer_idx = answer_index_cycle.pop(0)\n",
        "                        answer_index_cycle.append(answer_idx)\n",
        "                        q[\"answer\"] = q[\"options\"][answer_idx][0]\n",
        "                    else:\n",
        "                        q[\"answer\"] = \"A\"\n",
        "                    questions.append(q)\n",
        "                q = {\"question\": question_text, \"options\": [], \"answer\": \"A\"}\n",
        "            elif re.match(r\"^[A-D]\\.\", line):\n",
        "                q.setdefault(\"options\", []).append(line)\n",
        "        if q:\n",
        "            if q.get(\"options\"):\n",
        "                answer_idx = answer_index_cycle.pop(0)\n",
        "                answer_index_cycle.append(answer_idx)\n",
        "                q[\"answer\"] = q[\"options\"][answer_idx][0]\n",
        "            else:\n",
        "                q[\"answer\"] = \"A\"\n",
        "            questions.append(q)\n",
        "\n",
        "        return questions[:num_questions]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"LLM gagal membuat pertanyaan:\", e)\n",
        "        return []\n",
        "\n",
        "# Prepare assessment\n",
        "def prepare_assessment(detected_skills, tech_qs, total_questions=18):\n",
        "    num_skills = len(detected_skills)\n",
        "    questions_per_skill = total_questions // num_skills\n",
        "\n",
        "    assessment = {}\n",
        "    for skill in detected_skills:\n",
        "        questions = get_questions_for_skill_from_dataset(skill, tech_qs, num_questions=questions_per_skill)\n",
        "        if questions is None or len(questions) < questions_per_skill:\n",
        "            needed = questions_per_skill if questions is None else questions_per_skill - len(questions)\n",
        "            generated = generate_questions_gemini(skill, num_questions=needed)\n",
        "            questions = (questions or []) + generated\n",
        "\n",
        "        for i, q in enumerate(questions):\n",
        "            q.setdefault(\"question\", \"Pertanyaan tidak tersedia\")\n",
        "            q.setdefault(\"options\", [\"A. ...\", \"B. ...\", \"C. ...\", \"D. ...\"])\n",
        "            q.setdefault(\"answer\", \"A\")\n",
        "\n",
        "        assessment[skill] = questions\n",
        "\n",
        "    return assessment\n",
        "\n",
        "# Run assessment\n",
        "def run_assessment(assessment):\n",
        "    results = {}\n",
        "\n",
        "    for skill, questions in assessment.items():\n",
        "        print(f\"\\nSkill: {skill}\")\n",
        "        correct_count = 0\n",
        "        for idx, q in enumerate(questions, 1):\n",
        "            print(f\"\\n{idx}. {q['question']}\")\n",
        "\n",
        "            options = q.get('options', [\"A. ...\", \"B. ...\", \"C. ...\", \"D. ...\"])\n",
        "            for opt in options:\n",
        "                print(opt)\n",
        "\n",
        "            user_answer = input(\"Jawaban (A/B/C/D): \").strip().upper()\n",
        "            while user_answer not in ['A','B','C','D']:\n",
        "                user_answer = input(\"Jawaban salah. Masukkan A/B/C/D: \").strip().upper()\n",
        "\n",
        "            if user_answer == q.get('answer','A').upper():\n",
        "                correct_count += 1\n",
        "\n",
        "        if correct_count == len(questions):\n",
        "            level = \"Advanced\"\n",
        "        elif correct_count >= len(questions) - 1:\n",
        "            level = \"Intermediate\"\n",
        "        else:\n",
        "            level = \"Beginner\"\n",
        "\n",
        "        results[skill] = {\n",
        "            \"correct\": correct_count,\n",
        "            \"total\": len(questions),\n",
        "            \"level\": level\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Hasil\n",
        "user_input = input(\"Masukkan deskripsi user: \")\n",
        "\n",
        "job_role_detected = detect_job_role(user_input, job_role)\n",
        "skills_detected = detect_skills(user_input, job_role_detected, top_k=6)\n",
        "\n",
        "print(\"Job Role:\", job_role_detected)\n",
        "print(\"Skill:\", \", \".join(skills_detected))\n",
        "\n",
        "total_q = int(input(\"Pilih total pertanyaan (18 atau 36): \"))\n",
        "\n",
        "assessment = prepare_assessment(skills_detected, tech_qs, total_questions=total_q)\n",
        "results = run_assessment(assessment)\n",
        "\n",
        "print(\"\\nHasil Assessment\")\n",
        "for skill, res in results.items():\n",
        "    print(f\"{skill}: {res['correct']}/{res['total']} benar, Level: {res['level']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-time Recommendation & Guidance"
      ],
      "metadata": {
        "id": "HX8q-oga9jaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rekomendasi Course"
      ],
      "metadata": {
        "id": "7VZg4mBZqBUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rapidfuzz\n"
      ],
      "metadata": {
        "id": "qGbk8r2Z7bpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rapidfuzz import process, fuzz"
      ],
      "metadata": {
        "id": "ppGO75hQCEKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_user_level_majority(results):\n",
        "    # ambil semua level dari results\n",
        "    levels = [res[\"level\"] for res in results.values()]\n",
        "\n",
        "    # hitung frekuensi tiap level\n",
        "    level_count = Counter(levels)\n",
        "\n",
        "    # cari level yang paling banyak muncul\n",
        "    most_common_level, count = level_count.most_common(1)[0]\n",
        "\n",
        "    return most_common_level\n",
        "\n",
        "final_level = aggregate_user_level_majority(results)\n",
        "print(f\"\\nLevel User Berdasarkan Assessment: {final_level}\")"
      ],
      "metadata": {
        "id": "JZ7RnQYVJJFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN TEXT\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = text.replace(\"-\", \" \")\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# LOAD DATA\n",
        "course = lp_mapping['Course']\n",
        "\n",
        "learning_path_mapping = {\n",
        "    1: \"AI Engineer\",\n",
        "    2: \"Android Developer\",\n",
        "    3: \"Back-End Developer JavaScript\",\n",
        "    4: \"Back-End Developer Python\",\n",
        "    5: \"Data Scientist\",\n",
        "    6: \"DevOps Engineer\",\n",
        "    7: \"Front-End Web Developer\",\n",
        "    8: \"Gen AI Engineer\",\n",
        "    9: \"Google Cloud Professional\",\n",
        "    10: \"iOS Developer\",\n",
        "    11: \"MLOps Engineer\",\n",
        "    12: \"Multi-Platform App Developer\",\n",
        "    13: \"React Developer\"\n",
        "}\n",
        "\n",
        "course['learning_path'] = course['learning_path_id'].map(learning_path_mapping)\n",
        "course['course_level'] = course['course_level_str'].replace({\n",
        "    1: \"Beginner\",\n",
        "    2: \"Beginner\",\n",
        "    3: \"Intermediate\",\n",
        "    4: \"Advanced\",\n",
        "    5: \"Advanced\"\n",
        "})\n",
        "\n",
        "# FUZZY MATCH (langsung ke lp_answer)\n",
        "def fuzzy_match_into(df_left, df_right, left_col, right_col, threshold=70):\n",
        "\n",
        "    matched_list = []\n",
        "\n",
        "    # cari cocokannya\n",
        "    for value in df_left[left_col]:\n",
        "        if pd.isna(value):\n",
        "            matched_list.append(None)\n",
        "            continue\n",
        "\n",
        "        match = process.extractOne(\n",
        "            value,\n",
        "            df_right[right_col].tolist(),\n",
        "            scorer=fuzz.token_set_ratio\n",
        "        )\n",
        "\n",
        "        if match and match[1] >= threshold:\n",
        "            matched_list.append(match[0])\n",
        "        else:\n",
        "            matched_list.append(None)\n",
        "\n",
        "    # tambah kolom matched_name langsung ke lp_answer\n",
        "    df_left['matched_name'] = matched_list\n",
        "\n",
        "    # merge langsung tanpa copy\n",
        "    return df_left.merge(\n",
        "        df_right,\n",
        "        left_on='matched_name',\n",
        "        right_on=right_col,\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "# MERGE (tanpa normalize, tanpa copy)\n",
        "lp_combined = fuzzy_match_into(\n",
        "    lp_answer,\n",
        "    course[['course_name', 'course_level', 'learning_path']],\n",
        "    left_col='name',\n",
        "    right_col='course_name',\n",
        "    threshold=70\n",
        ")\n",
        "\n",
        "# COMBINED TEXT\n",
        "lp_combined['combined_text'] = (\n",
        "    lp_combined['course_level'].fillna('') + \" \" +\n",
        "    lp_combined['learning_path'].fillna('') + \" \" +\n",
        "    lp_combined['course_name'].fillna('') + \" \" +\n",
        "    lp_combined['description'].fillna('') + \" \" +\n",
        "    lp_combined['technologies'].fillna('')\n",
        ").apply(clean_text)\n",
        "\n",
        "# LOAD MODEL\n",
        "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "course_embeddings = model.encode(lp_combined['combined_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# RECOMMEND FUNCTION\n",
        "def recommend_from_user_job(user_job, user_level=None, top_k=1):\n",
        "\n",
        "    final_input = f\"{user_job} {user_level}\" if user_level else user_job\n",
        "    input_clean = clean_text(final_input)\n",
        "    user_emb = model.encode([input_clean])\n",
        "\n",
        "    sim_scores = cosine_similarity(user_emb, course_embeddings).flatten()\n",
        "\n",
        "    df = lp_combined.copy()\n",
        "    df['sim'] = sim_scores\n",
        "\n",
        "    # filter LP\n",
        "    matched_lp = None\n",
        "    for lp in learning_path_mapping.values():\n",
        "        if lp.lower() in user_job.lower():\n",
        "            matched_lp = lp\n",
        "            break\n",
        "\n",
        "    if matched_lp:\n",
        "        df = df[df['learning_path'] == matched_lp]\n",
        "\n",
        "    # filter level\n",
        "    if user_level:\n",
        "        df = df[df['course_level'].str.lower() == user_level.lower()]\n",
        "        df = df.sort_values(by=\"sim\", ascending=False).head(top_k)\n",
        "    return df[['course_name','course_level', 'summary', 'course_price']]\n",
        "\n",
        "# TEST\n",
        "user_job = job_role_detected\n",
        "user_level = final_level\n",
        "\n",
        "recommend_from_user_job(user_job, user_level)"
      ],
      "metadata": {
        "id": "RieQmHKf8HeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saran strategi belajar, resource eksternal, hingga jadwal belajar optimal"
      ],
      "metadata": {
        "id": "O7b6Ybyx9rbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "4labUu1TCPhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA\n",
        "df = pd.read_csv(\"Skill.csv\").fillna(\"\")\n",
        "level_order = {\"Beginner\": 1, \"Intermediate\": 2, \"Advanced\": 3}\n",
        "df[\"level_num\"] = df[\"skill_level\"].map(level_order)\n",
        "\n",
        "# TF-IDF\n",
        "df[\"skill_text\"] = df[\"skill\"] + \" \" + df[\"description\"]\n",
        "tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
        "skill_tfidf = tfidf.fit_transform(df[\"skill_text\"])\n",
        "\n",
        "df[\"lp_text\"] = df[\"learning_path_name\"] + \" \" + df[\"skill_text\"]\n",
        "lp_tfidf = tfidf.fit_transform(df[\"lp_text\"])\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def parse_prereq(prereq):\n",
        "    if prereq == \"\":\n",
        "        return []\n",
        "    text = prereq.lower()\n",
        "    text = re.sub(r\" and |&|\\+|/|;\", \",\", text)\n",
        "    parts = [p.strip() for p in text.split(\",\") if p.strip()]\n",
        "    return parts\n",
        "\n",
        "def match_skill(token):\n",
        "    token = token.lower()\n",
        "    for s in df[\"skill\"].str.lower():\n",
        "        s_clean = s.split(\"(\")[0].strip()\n",
        "        if token == s_clean or token in s_clean or s_clean in token:\n",
        "            return s_clean\n",
        "    return None\n",
        "\n",
        "def detect_learning_path(query):\n",
        "    q_vec = tfidf.transform([query])\n",
        "    lp_scores = {}\n",
        "    for lp in df[\"learning_path_name\"].unique():\n",
        "        lp_skills = df[df[\"learning_path_name\"] == lp]\n",
        "        lp_vec = tfidf.transform(lp_skills[\"skill_text\"])\n",
        "        sims = cosine_similarity(q_vec, lp_vec).max()\n",
        "        lp_scores[lp] = sims\n",
        "    best_lp = max(lp_scores, key=lp_scores.get)\n",
        "    return best_lp\n",
        "\n",
        "def find_current_skill(query):\n",
        "    lp = detect_learning_path(query)\n",
        "    df_filtered = df[df[\"learning_path_name\"] == lp].copy()\n",
        "    q_lower = query.lower()\n",
        "\n",
        "    for s in df_filtered[\"skill\"].str.lower():\n",
        "        s_clean = s.split(\"(\")[0].strip()\n",
        "        if re.search(r\"\\b\" + re.escape(s_clean) + r\"\\b\", q_lower):\n",
        "            match_df = df_filtered[df_filtered[\"skill\"].str.lower() == s]\n",
        "            if not match_df.empty:\n",
        "                return match_df.iloc[0]\n",
        "\n",
        "    if not df_filtered.empty:\n",
        "        return df_filtered.iloc[df_filtered[\"level_num\"].idxmin()]\n",
        "\n",
        "    return None\n",
        "\n",
        "def find_current_skills(query):\n",
        "    lp = detect_learning_path(query)\n",
        "    df_filtered = df[df[\"learning_path_name\"] == lp].copy()\n",
        "    query_lower = query.lower()\n",
        "    matched_skills = []\n",
        "\n",
        "    for s in df_filtered[\"skill\"].str.lower():\n",
        "        s_clean = s.split(\"(\")[0].strip()\n",
        "        if re.search(r\"\\b\" + re.escape(s_clean) + r\"\\b\", query_lower):\n",
        "            match_df = df_filtered[df_filtered[\"skill\"].str.lower() == s]\n",
        "            if not match_df.empty:\n",
        "                matched_skills.append(match_df.iloc[0])\n",
        "\n",
        "    if not matched_skills:\n",
        "        curr_skill = find_current_skill(query)\n",
        "        if curr_skill is not None:\n",
        "            matched_skills.append(curr_skill)\n",
        "\n",
        "    if matched_skills:\n",
        "        print(f\"Detected Learning Path: {lp}\")\n",
        "        print(f\"Detected Current Skills: {[s['skill'] for s in matched_skills]}\")\n",
        "    else:\n",
        "        print(f\"Tidak ada skill yang terdeteksi di learning path: {lp}\")\n",
        "\n",
        "    return matched_skills\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "pairs = []\n",
        "for _, row in df.iterrows():\n",
        "    curr = row[\"skill\"]\n",
        "    curr_level = row[\"level_num\"]\n",
        "    prereq_tokens = parse_prereq(row[\"prerequisite\"])\n",
        "    if not prereq_tokens:\n",
        "        continue\n",
        "    for p in prereq_tokens:\n",
        "        matched_skill = match_skill(p)\n",
        "        if matched_skill is None:\n",
        "            continue\n",
        "        parent_rows = df[df[\"skill\"].str.lower().str.startswith(matched_skill)]\n",
        "        if parent_rows.empty:\n",
        "            continue\n",
        "        parent = parent_rows.iloc[0]\n",
        "        pairs.append({\n",
        "            \"current_skill\": parent[\"skill\"],\n",
        "            \"next_skill\": curr,\n",
        "            \"current_level\": parent[\"level_num\"],\n",
        "            \"next_level\": curr_level,\n",
        "            \"is_prerequisite\": 1\n",
        "        })\n",
        "\n",
        "positive_pairs = pd.DataFrame(pairs)\n",
        "\n",
        "# NEGATIVE SAMPLING\n",
        "neg_pairs = []\n",
        "for _ in range(len(positive_pairs) // 2):\n",
        "    lp = np.random.choice(df[\"learning_path_name\"].unique())\n",
        "    lp_skills = df[df[\"learning_path_name\"] == lp]\n",
        "    if len(lp_skills) >= 2:\n",
        "        sample = lp_skills.sample(2).reset_index(drop=True)\n",
        "        a, b = sample.iloc[0], sample.iloc[1]\n",
        "        # Pastikan bukan prerequisite yang valid\n",
        "        if a[\"skill\"] not in b[\"prerequisite\"]:\n",
        "            neg_pairs.append({\n",
        "                \"current_skill\": a[\"skill\"],\n",
        "                \"next_skill\": b[\"skill\"],\n",
        "                \"current_level\": a[\"level_num\"],\n",
        "                \"next_level\": b[\"level_num\"],\n",
        "                \"is_prerequisite\": 0\n",
        "            })\n",
        "\n",
        "# Different learning paths\n",
        "for _ in range(len(positive_pairs) // 2):\n",
        "    lp1, lp2 = np.random.choice(df[\"learning_path_name\"].unique(), 2, replace=False)\n",
        "    a = df[df[\"learning_path_name\"] == lp1].sample(1).iloc[0]\n",
        "    b = df[df[\"learning_path_name\"] == lp2].sample(1).iloc[0]\n",
        "    neg_pairs.append({\n",
        "        \"current_skill\": a[\"skill\"],\n",
        "        \"next_skill\": b[\"skill\"],\n",
        "        \"current_level\": a[\"level_num\"],\n",
        "        \"next_level\": b[\"level_num\"],\n",
        "        \"is_prerequisite\": 0\n",
        "    })\n",
        "\n",
        "negative_pairs = pd.DataFrame(neg_pairs)\n",
        "pairs_df = pd.concat([positive_pairs, negative_pairs]).reset_index(drop=True)\n",
        "\n",
        "# FEATURE EXTRACTION\n",
        "X_features = []\n",
        "y_labels = pairs_df[\"is_prerequisite\"].values\n",
        "\n",
        "for _, row in pairs_df.iterrows():\n",
        "    try:\n",
        "        curr_mask = df[\"skill\"] == row[\"current_skill\"]\n",
        "        next_mask = df[\"skill\"] == row[\"next_skill\"]\n",
        "\n",
        "        if not curr_mask.any() or not next_mask.any():\n",
        "            continue\n",
        "\n",
        "        curr_idx = df[curr_mask].index[0]\n",
        "        next_idx = df[next_mask].index[0]\n",
        "\n",
        "        curr_row = df.iloc[curr_idx]\n",
        "        next_row = df.iloc[next_idx]\n",
        "\n",
        "        # Cosine similarity\n",
        "        sim = cosine_similarity(skill_tfidf[curr_idx], skill_tfidf[next_idx])[0][0]\n",
        "\n",
        "        # Level difference\n",
        "        level_diff = row[\"next_level\"] - row[\"current_level\"]\n",
        "\n",
        "        # Same learning path indicator\n",
        "        same_lp = 1 if curr_row[\"learning_path_name\"] == next_row[\"learning_path_name\"] else 0\n",
        "\n",
        "        # Prerequisite string matching\n",
        "        prereqs = parse_prereq(next_row[\"prerequisite\"])\n",
        "        curr_skill_name = curr_row[\"skill\"].lower().split(\"(\")[0].strip()\n",
        "        prereq_match = 0\n",
        "        for p in prereqs:\n",
        "            matched = match_skill(p)\n",
        "            if matched and curr_skill_name in matched:\n",
        "                prereq_match = 1\n",
        "                break\n",
        "\n",
        "        # Level progression validity (1 if valid progression, 0 otherwise)\n",
        "        valid_progression = 1 if level_diff >= 0 and level_diff <= 2 else 0\n",
        "\n",
        "        X_features.append([\n",
        "            sim,\n",
        "            level_diff,\n",
        "            row[\"current_level\"],\n",
        "            row[\"next_level\"],\n",
        "            same_lp,\n",
        "            prereq_match,\n",
        "            valid_progression\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "X = np.array(X_features)\n",
        "y = y_labels[:len(X_features)]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# MODEL\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=3,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(f\"Train accuracy: {rf_model.score(X_train, y_train):.3f}\")\n",
        "print(f\"Test accuracy: {rf_model.score(X_test, y_test):.3f}\")\n",
        "\n",
        "# CLASSIFICATION REPORT\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    target_names=[\"Not Prerequisite\", \"Prerequisite\"]\n",
        "))"
      ],
      "metadata": {
        "id": "OF4d5H0-8JAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICT NEXT SKILLS\n",
        "def predict_next_skills(query, top_n=5):\n",
        "    current_skills = find_current_skills(query)\n",
        "    if not current_skills:\n",
        "        print(\"Tidak ada skill saat ini yang terdeteksi untuk query ini.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    current_skill_names = set()\n",
        "    for curr in current_skills:\n",
        "        skill_lower = curr[\"skill\"].lower().split(\"(\")[0].strip()\n",
        "        current_skill_names.add(skill_lower)\n",
        "\n",
        "    lp = current_skills[0][\"learning_path_name\"]\n",
        "    max_curr_level = max(curr[\"level_num\"] for curr in current_skills)\n",
        "\n",
        "    lp_skills = df[df[\"learning_path_name\"] == lp].copy()\n",
        "\n",
        "    def prereqs_satisfied(row):\n",
        "        prereq_tokens = parse_prereq(row[\"prerequisite\"])\n",
        "        if not prereq_tokens:\n",
        "            return False\n",
        "\n",
        "        matched_prereqs = set()\n",
        "        for token in prereq_tokens:\n",
        "            matched = match_skill(token)\n",
        "            if matched:\n",
        "                matched_prereqs.add(matched)\n",
        "\n",
        "        if len(matched_prereqs) == 0:\n",
        "            return False\n",
        "\n",
        "        matched_count = 0\n",
        "        for curr_name in current_skill_names:\n",
        "            for prereq in matched_prereqs:\n",
        "                if curr_name in prereq or prereq in curr_name:\n",
        "                    matched_count += 1\n",
        "                    break\n",
        "\n",
        "        if len(current_skill_names) == 1:\n",
        "            return matched_count >= 1\n",
        "        else:\n",
        "            return matched_count == len(current_skill_names)\n",
        "\n",
        "    candidates = lp_skills[lp_skills.apply(prereqs_satisfied, axis=1)]\n",
        "    candidates = candidates[candidates[\"level_num\"] >= max_curr_level]\n",
        "\n",
        "    for _, cand in candidates.iterrows():\n",
        "        try:\n",
        "            next_idx = cand.name\n",
        "            similarities = []\n",
        "\n",
        "            for curr in current_skills:\n",
        "                curr_idx = curr.name\n",
        "                if curr_idx not in df.index or next_idx not in df.index:\n",
        "                    continue\n",
        "                sim = cosine_similarity(skill_tfidf[curr_idx], skill_tfidf[next_idx])[0][0]\n",
        "                similarities.append(sim)\n",
        "\n",
        "            if not similarities:\n",
        "                continue\n",
        "\n",
        "            avg_sim = np.mean(similarities)\n",
        "\n",
        "            level_diff = cand[\"level_num\"] - max_curr_level\n",
        "            same_lp = 1\n",
        "\n",
        "            prereqs = parse_prereq(cand[\"prerequisite\"])\n",
        "            prereq_match = 0\n",
        "            for p in prereqs:\n",
        "                matched = match_skill(p)\n",
        "                if matched:\n",
        "                    for curr_name in current_skill_names:\n",
        "                        if curr_name in matched or matched in curr_name:\n",
        "                            prereq_match = 1\n",
        "                            break\n",
        "                if prereq_match:\n",
        "                    break\n",
        "\n",
        "            valid_progression = 1 if level_diff >= 0 and level_diff <= 2 else 0\n",
        "\n",
        "            features = np.array([[\n",
        "                avg_sim,\n",
        "                level_diff,\n",
        "                max_curr_level,\n",
        "                cand[\"level_num\"],\n",
        "                same_lp,\n",
        "                prereq_match,\n",
        "                valid_progression\n",
        "            ]])\n",
        "            prob = rf_model.predict_proba(features)[0][1]\n",
        "\n",
        "            level_boost = 1.0\n",
        "            if cand[\"level_num\"] == max_curr_level + 1:\n",
        "                level_boost = 1.2\n",
        "            elif cand[\"level_num\"] == max_curr_level:\n",
        "                level_boost = 1.1\n",
        "\n",
        "            all_predictions.append({\n",
        "                \"skill\": cand[\"skill\"],\n",
        "                \"skill_level\": cand[\"skill_level\"],\n",
        "                \"prerequisite\": cand[\"prerequisite\"],\n",
        "                \"probability\": prob * level_boost,\n",
        "                \"level_num\": cand[\"level_num\"]\n",
        "            })\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if not all_predictions:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    result_df = pd.DataFrame(all_predictions)\n",
        "    result_df[\"total_prereq\"] = result_df[\"prerequisite\"].apply(lambda x: len(parse_prereq(x)))\n",
        "\n",
        "    result_df = result_df.sort_values(\n",
        "        [\"level_num\", \"total_prereq\", \"probability\"],\n",
        "        ascending=[True, True, False]\n",
        "    )\n",
        "\n",
        "    result_df = result_df.drop(columns=[\"level_num\", \"total_prereq\"])\n",
        "    result_df = result_df.drop_duplicates(subset=[\"skill\"])\n",
        "    return result_df.head(top_n)\n",
        "\n",
        "\n",
        "# TESTING\n",
        "test_queries = [\n",
        "    \"sehabis HTML, css apa lagi yang harus dipelajari untuk jadi web developer?\",\n",
        "    \"sehabis JavaScript, apa lagi yang harus dipelajari untuk jadi Front-End Web Developer?\",\n",
        "    \"sehabis React, apa lagi yang harus dipelajari untuk jadi Front-End Web Developer?\"\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    print(f\"Query: {q}\")\n",
        "    result = predict_next_skills(q, top_n=5)\n",
        "    if not result.empty:\n",
        "        print(result.to_string(index=False))\n",
        "    else:\n",
        "        print(\"No recommendations found.\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "PXTJ767c8lzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Load data.json\n",
        "with open(\"data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "resources = data[\"resources\"]\n",
        "\n",
        "# Setup LLM Gemini\n",
        "genai.configure(api_key=\"AIzaSyAz2OsOS2RhdlEZNzjJxmXDVbV4OH-_FJQ\")\n",
        "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
        "\n",
        "# Fungsi Generate Strategi Belajar\n",
        "def generate_actionable_learning_strategy(next_skills, goal=None):\n",
        "    next_text = \", \".join(next_skills)\n",
        "\n",
        "    level_nums = []\n",
        "    for skill in next_skills:\n",
        "        row = df[df[\"skill\"] == skill]\n",
        "        if not row.empty:\n",
        "            level_nums.append(row[\"level_num\"].iloc[0])\n",
        "    level_num = min(level_nums) if level_nums else 1\n",
        "\n",
        "    # Ambil maksimal 2 resource per skill\n",
        "    references_text = \"\"\n",
        "    for i, skill in enumerate(next_skills, start=1):\n",
        "        # Filter resource yang judulnya mengandung skill (jika field skill tidak ada)\n",
        "        skill_resources = [r for r in resources if skill.lower() in r.get(\"title\", \"\").lower()][:2]\n",
        "        if skill_resources:\n",
        "            references_text += f\"{i}. {skill}:\\n\"\n",
        "            for r in skill_resources:\n",
        "                references_text += f\"  - {r['title']} ({r['type']})\\n    {r['url']}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Kamu adalah mentor belajar yang praktis dan terstruktur. Gunakan bahasa yang tidak terlalu baku, semangat, ramah, dan ceria.\n",
        "Buat strategi belajar harian untuk seseorang yang ingin mempelajari skill: {next_text} {f\"dengan tujuan akhir: {goal}\" if goal else \"\"}.\n",
        "Berikan saran output subskill berdasarkan level skill yang sudah terdeteksi: {level_nums}.\n",
        "Format output harus persis seperti ini.\n",
        "Yuk naikin skill kamu! Ini rencana belajar yang bakal nge-boost perkembanganmu! âš¡\n",
        "1ï¸. <Nama Skill>:\n",
        "  - <Subskill>\n",
        "  - <Subskill>\n",
        "  - <Subskill>\n",
        "  dst.\n",
        "\n",
        "Tips seru supaya belajar makin efektif ðŸ’¡\n",
        "(Pilih 3 tips belajar secara acak. Harus berikan 1 tips teknik belajar populer.\n",
        "Tips boleh berasal dari kebiasaan belajar, trik fokus, pengaturan lingkungan, manajemen waktu, mindset positif, atau cara mencatat yang efektif.\n",
        "Setiap tips harus sangat singkat, maksimal 2 kalimat pendek, tidak lebih dari 10 kata.)\n",
        "- <Tips 1>\n",
        "- <Tips 2>\n",
        "- <Tips 3>\n",
        "\n",
        "Referensi buat kamu ðŸ“š\n",
        "(Resource yang dikeluarkan hanya berdasarkan skill: {next_text}. Dilarang menambahkan resource selain di sini)\n",
        "{references_text}\n",
        "\n",
        "Berikut roadmap belajar mingguanmu ðŸ˜‰\n",
        "Atur jadwal sesuai aturan berikut:\n",
        "1. Fokus pada satu skill utama terlebih dahulu sebelum pindah ke skill berikutnya.\n",
        "2. Jika dalam satu hari hanya ada 1 subskill â†’ berikan 1 durasi belajar (durasi bebas).\n",
        "3. Jika dalam satu hari ada lebih dari 1 subskill â†’ berikan durasi untuk masing-masing subskill (setiap durasi bebas).\n",
        "4. Durasi belajar harus realistis dan logis berdasarkan tingkat kesulitan subskill.\n",
        "5. Durasi belajar menggunakan format lama waktu, bukan jam pada pukul tertentu.\n",
        "Durasi belajar harus dalam format lama waktu saja: \"45 menit\", \"1 jam\", \"90 menit\", \"2 jam\", dst. Tidak boleh menggunakan kombinasi jam + menit (misal: \"1 jam 30 menit\" dilarang).\n",
        "Format jadwal harus persis seperti ini:\n",
        "- Hari 1: <Durasi 1> â†’ <Skill>: <Subskill 1>\n",
        "  Hari 1: <Durasi 2> â†’ <Skill>: <Subskill 2> (jika ada)\n",
        "- Hari 2: <Durasi> â†’ <Skill>: <Subskill>\n",
        "dst.\n",
        "\"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "# Full Pipeline Example\n",
        "query = \"sehabis css, apa lagi yang harus dipelajari untuk jadi front-end web developer?\"\n",
        "next_skills_df = predict_next_skills(query, top_n=5)\n",
        "\n",
        "if not next_skills_df.empty:\n",
        "    next_skills = [s.split(\"(\")[0].strip() for s in next_skills_df[\"skill\"].tolist()]\n",
        "    strategy = generate_actionable_learning_strategy(next_skills)\n",
        "    print(strategy.replace(\"**\", \"\"))\n",
        "else:\n",
        "    print(\"No next skill recommendations found.\")\n"
      ],
      "metadata": {
        "id": "MnTyVczUHRl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roadmap Course (Roadmap Course.xlsx)"
      ],
      "metadata": {
        "id": "X1yTO3Vg-OMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders catboost\n"
      ],
      "metadata": {
        "id": "kZWbKmjt-oYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    mean_squared_error, r2_score\n",
        ")\n",
        "from scipy.stats import randint\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "import category_encoders as ce\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "5CMXegayCscG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "roadmap = pd.ExcelFile('Roadmap Course.xlsx')\n",
        "user_prog = pd.read_excel('Roadmap Course.xlsx', sheet_name='User Progress')\n",
        "\n",
        "# Tentukan kolom categorical & numerical\n",
        "categorical_cols = ['user_id', 'title_id']\n",
        "numerical_cols = [col for col in user_prog.columns\n",
        "                  if col not in categorical_cols + ['progress_percentage', 'status']]\n",
        "\n",
        "# Feature Engineering\n",
        "# Fitur kombinasi user_id x title_id\n",
        "user_prog['user_title'] = user_prog['user_id'].astype(str) + '_' + user_prog['title_id'].astype(str)\n",
        "categorical_cols.append('user_title')\n",
        "\n",
        "# Pisahkan X dan y\n",
        "X = user_prog[categorical_cols + numerical_cols].copy()\n",
        "y_reg = user_prog['progress_percentage']\n",
        "y_cls = user_prog['status']\n",
        "\n",
        "# Target Encoding pada fitur kategorikal\n",
        "encoder = ce.TargetEncoder(cols=categorical_cols)\n",
        "X[categorical_cols] = encoder.fit_transform(X[categorical_cols], y_reg)\n",
        "\n",
        "# Split train-test\n",
        "# Regresi\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Klasifikasi dengan stratify\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X, y_cls, test_size=0.2, random_state=42, stratify=y_cls\n",
        ")\n",
        "\n",
        "# SMOTE untuk klasifikasi\n",
        "smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "X_train_cls_res, y_train_cls_res = smote.fit_resample(X_train_cls, y_train_cls)\n",
        "\n",
        "print(\"\\nDistribusi sebelum SMOTE:\\n\", y_train_cls.value_counts())\n",
        "print(\"\\nDistribusi sesudah SMOTE:\\n\", y_train_cls_res.value_counts())\n",
        "\n",
        "# Model Regresi: CatBoost\n",
        "regressor = CatBoostRegressor(\n",
        "    iterations=2000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    loss_function='RMSE',\n",
        "    verbose=0,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "# Early stopping pool\n",
        "reg_pool = Pool(X_train_reg, y_train_reg)\n",
        "regressor.fit(reg_pool, eval_set=(X_test_reg, y_test_reg), early_stopping_rounds=50)\n",
        "\n",
        "# Hyperparameter Tuning RandomForest (Klasifikasi)\n",
        "param_dist = {\n",
        "    \"n_estimators\": randint(500, 1500),\n",
        "    \"max_depth\": randint(15, 35),\n",
        "    \"min_samples_split\": randint(3, 10),\n",
        "    \"min_samples_leaf\": randint(2, 5),\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"bootstrap\": [True, False],\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    cv=3,\n",
        "    verbose=0,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"f1_macro\"\n",
        ")\n",
        "\n",
        "search.fit(X_train_cls_res, y_train_cls_res)\n",
        "best_rf = search.best_estimator_\n",
        "\n",
        "# Prediksi\n",
        "# Regresi\n",
        "y_pred_reg = regressor.predict(X_test_reg)\n",
        "\n",
        "# Klasifikasi\n",
        "y_pred_cls = best_rf.predict(X_test_cls)\n",
        "y_train_pred_cls = best_rf.predict(X_train_cls_res)\n",
        "\n",
        "# Evaluasi Model\n",
        "# Regresi\n",
        "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(\"\\nEvaluasi Regresi (CatBoost)\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2:\", r2)\n",
        "\n",
        "# Klasifikasi\n",
        "train_acc = accuracy_score(y_train_cls_res, y_train_pred_cls)\n",
        "test_acc = accuracy_score(y_test_cls, y_pred_cls)\n",
        "\n",
        "print(\"\\nEvaluasi Klasifikasi (RandomForest)\")\n",
        "print(\"Train Accuracy:\", train_acc)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_cls, y_pred_cls))"
      ],
      "metadata": {
        "id": "3DHTDc8xGKzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_cls, y_pred_cls)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9JMxGkFV8eGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "import joblib\n",
        "joblib.dump(regressor, 'catboost_progress.pkl')\n",
        "joblib.dump(best_rf, 'rf_status.pkl')\n",
        "joblib.dump(encoder, 'target_encoder.pkl')"
      ],
      "metadata": {
        "id": "liOzv6Mp26ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df_users = pd.read_excel('Roadmap Course.xlsx', sheet_name='User')\n",
        "df_progress = pd.read_excel('Roadmap Course.xlsx', sheet_name='User Progress')\n",
        "df_roadmap = pd.read_excel('Roadmap Course.xlsx', sheet_name='Title')\n",
        "\n",
        "# Tentukan kolom categorical & numerical\n",
        "categorical_cols = ['user_id', 'title_id', 'user_title']\n",
        "numerical_cols = [col for col in df_progress.columns if col not in ['user_id', 'title_id', 'user_title', 'progress_percentage', 'status']]\n",
        "\n",
        "# Load model & encoder\n",
        "regressor = joblib.load('catboost_progress.pkl')\n",
        "best_rf = joblib.load('rf_status.pkl')\n",
        "encoder = joblib.load('target_encoder.pkl')\n",
        "\n",
        "# Feature Engineering untuk prediksi\n",
        "df_progress['user_title'] = df_progress['user_id'].astype(str) + '_' + df_progress['title_id'].astype(str)\n",
        "\n",
        "# Buat X_all\n",
        "X_all = df_progress[categorical_cols + numerical_cols].copy()\n",
        "\n",
        "# Transform categorical features\n",
        "X_all[categorical_cols] = pd.DataFrame(\n",
        "    encoder.transform(X_all[categorical_cols]),\n",
        "    columns=categorical_cols\n",
        ")\n",
        "\n",
        "# Prediksi progress & status\n",
        "df_progress['pred_progress'] = regressor.predict(X_all)\n",
        "df_progress['pred_status'] = best_rf.predict(X_all)\n",
        "\n",
        "# Merge dengan roadmap & user info\n",
        "df_all = pd.merge(df_progress, df_roadmap, on='title_id', how='left')\n",
        "df_all = pd.merge(df_all, df_users[['user_id','email','name','course','learning_path_name']],\n",
        "                  on=['user_id','course','learning_path_name'], how='left')\n",
        "\n",
        "# Tentukan unlock modul berikutnya\n",
        "def unlock_logic(row):\n",
        "    if row['pred_progress'] >= row['unlock_requirement']:\n",
        "        return 'ðŸ”“ (Unlocked)'\n",
        "    else:\n",
        "        return 'ðŸ”’ (Locked)'\n",
        "\n",
        "df_all['next_access'] = df_all.apply(unlock_logic, axis=1)\n",
        "\n",
        "def format_roadmap(row):\n",
        "    if row['pred_status'] == 'Completed':\n",
        "        return f\"{row['title']} âœ”ï¸ (Completed)\"\n",
        "    elif row['pred_status'] == 'In Progress':\n",
        "        return f\"{row['title']} {int(round(row['pred_progress']))}% (In Progress)\"\n",
        "    else:\n",
        "        return f\"{row['title']} {row['next_access']}\"\n",
        "\n",
        "df_all['roadmap_view'] = df_all.apply(format_roadmap, axis=1)\n",
        "\n",
        "# Print roadmap per user\n",
        "for (user_id, email, course, learning_path), group in df_all.groupby(['user_id','email','course','learning_path_name']):\n",
        "    user_name = group.iloc[0]['name']\n",
        "    print(f\"\\nRoadmap Diperbarui untuk {user_name} ({email})\\nCourse: {course}\\nLearning Path: {learning_path}\\n\")\n",
        "\n",
        "    # Print roadmap urut per title_id\n",
        "    for i, line in enumerate(group.sort_values('title_id')['roadmap_view'], start=1):\n",
        "        print(f\"{i}. {line}\")\n",
        "\n",
        "    # Modul berikutnya yang terbuka\n",
        "    in_prog = group[(group['pred_status']=='In Progress') & (group['pred_progress'] >= group['unlock_requirement'])]\n",
        "    if not in_prog.empty:\n",
        "        next_id = in_prog.iloc[0]['next_title_id']\n",
        "        next_title = group[group['title_id']==next_id]['title'].values\n",
        "        if len(next_title):\n",
        "            print(f\"\\nKarena kamu sudah mencapai {round(in_prog.iloc[0]['pred_progress'],2)}% pada {in_prog.iloc[0]['title']}, \"\n",
        "                  f\"modul {next_title[0]} kini terbuka untukmu.\")"
      ],
      "metadata": {
        "id": "gLOZmU7GH6c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Personal Learning Assistant"
      ],
      "metadata": {
        "id": "agVeB1jD_Wk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skill apa yang paling berkembang minggu ini?"
      ],
      "metadata": {
        "id": "YIGHTawhzTbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install iterative-stratification"
      ],
      "metadata": {
        "id": "XkqEF8MaYXKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, hamming_loss"
      ],
      "metadata": {
        "id": "8O_m_AoEC3gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "PATH = \"Personal Learning Assistant.xlsx\"\n",
        "RANDOM_STATE = 42\n",
        "BASE_THRESHOLD = 0.2\n",
        "SCALE_THRESHOLD = 0.8\n",
        "\n",
        "# STOPWORDS\n",
        "custom_stopwords = [\n",
        "    'dan','dengan','untuk','pada','dari','yang','ke','di','sebagai','dalam','atas','tentang',\n",
        "    'memulai','pemrograman','pengenalan','belajar','lanjutan', 'menjadi',\n",
        "    'modul','kelas','course','materi', 'tingkat','level','bagian','seri',\n",
        "    'introduction','learning','tutorial', 'module','class','overview','guide','getting','started','how','to','using',\n",
        "    'membangun','membuat','implementasi','aplikasi','project','studi','kasus',\n",
        "    'mengenal','bagaimana','cara','menggunakan','pengantar','praktik'\n",
        "]\n",
        "\n",
        "# FUNCTIONS\n",
        "def clean_text(text):\n",
        "    text = \"\" if pd.isna(text) else str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s\\+\\#\\-\\.]', ' ', text)\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "# LOAD DATA\n",
        "df = pd.read_excel(PATH, sheet_name=\"Skill Course\")\n",
        "\n",
        "learning_path_mapping = {\n",
        "    1: \"AI Engineer\", 2: \"Android Developer\", 3: \"Back-End Developer JavaScript\",\n",
        "    4: \"Back-End Developer Python\", 5: \"Data Scientist\", 6: \"DevOps Engineer\",\n",
        "    7: \"Front-End Web Developer\", 8: \"Gen AI Engineer\", 9: \"Google Cloud Professional\",\n",
        "    10: \"iOS Developer\", 11: \"MLOps Engineer\", 12: \"Multi-Platform App Developer\",\n",
        "    13: \"React Developer\"\n",
        "}\n",
        "\n",
        "# pastikan nama kolom yang dipakai tidak kosong\n",
        "df['learning_path_name'] = df['learning_path_id'].map(learning_path_mapping)\n",
        "df['course_name'] = df['course_name'].fillna('')\n",
        "df['learning_path_name'] = df['learning_path_name'].fillna('')\n",
        "\n",
        "# parsing skill (harus berbentuk list)\n",
        "df['skill'] = df['skill'].apply(\n",
        "    lambda x: [] if pd.isna(x) else [\n",
        "        s.strip().lower() for s in str(x).split(',')\n",
        "        if s and s.strip().lower() != 'nan' and s.strip()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# OVERSAMPLING MINORITAS\n",
        "all_skills = [s for skills in df['skill'] for s in skills]\n",
        "skill_counts = Counter(all_skills)\n",
        "minor_skills = [s for s, c in skill_counts.items() if c < 5]\n",
        "oversampled_rows = []\n",
        "\n",
        "for idx, skills in enumerate(df['skill']):\n",
        "    if any(s in minor_skills for s in skills):\n",
        "        oversampled_rows.append(df.iloc[idx])\n",
        "\n",
        "if oversampled_rows:\n",
        "    df = pd.concat([df, pd.DataFrame(oversampled_rows)], ignore_index=True)\n",
        "\n",
        "# TEXT BUILDING\n",
        "df['text'] = (\n",
        "    (df['learning_path_name'].apply(clean_text) + ' ') * 2 +\n",
        "    (df['course_name'].apply(clean_text) + ' ') * 8 +\n",
        "    (df['skill'].apply(lambda skills: ' '.join(skills)) + ' ') * 2\n",
        ")\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=3000,\n",
        "    ngram_range=(1,3),\n",
        "    min_df=1,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    norm='l2',\n",
        "    stop_words=custom_stopwords\n",
        ")\n",
        "X = tfidf.fit_transform(df['text'])\n",
        "\n",
        "# LABELS\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(df['skill'])\n",
        "\n",
        "# SPLIT DATA\n",
        "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "train_index, test_index = next(mskf.split(X, y))\n",
        "X_train, X_test = X[train_index], X[test_index]\n",
        "y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "# MODEL\n",
        "lr_model = OneVsRestClassifier(\n",
        "    LogisticRegression(\n",
        "        C=1.0,\n",
        "        penalty='l2',\n",
        "        solver='lbfgs',\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced',\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    n_jobs=-1\n",
        ")\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# PREDIKSI\n",
        "# TRAIN\n",
        "y_train_proba = 1 / (1 + np.exp(-lr_model.decision_function(X_train)))\n",
        "class_thresholds = BASE_THRESHOLD + SCALE_THRESHOLD * y_train_proba.mean(axis=0)\n",
        "y_train_pred = (y_train_proba > class_thresholds).astype(int)\n",
        "\n",
        "# TEST\n",
        "y_test_proba = 1 / (1 + np.exp(-lr_model.decision_function(X_test)))\n",
        "y_test_pred = (y_test_proba > class_thresholds).astype(int)\n",
        "\n",
        "# EVALUASI\n",
        "print(\"F1 SCORE\")\n",
        "print(f\"Train F1 (micro): {f1_score(y_train, y_train_pred, average='micro'):.4f}\")\n",
        "print(f\"Test  F1 (micro): {f1_score(y_test, y_test_pred, average='micro'):.4f}\")\n",
        "\n",
        "print(\"\\nAKURASI\")\n",
        "train_hamming_acc = 1 - hamming_loss(y_train, y_train_pred)\n",
        "test_hamming_acc = 1 - hamming_loss(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Train Hamming Accuracy: {train_hamming_acc:.4f}\")\n",
        "print(f\"Test  Hamming Accuracy: {test_hamming_acc:.4f}\")"
      ],
      "metadata": {
        "id": "vW-wKShCYfPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAPPING SKILL INDEX\n",
        "mlb_classes = list(mlb.classes_)\n",
        "class_to_index = {c: i for i, c in enumerate(mlb_classes)}\n",
        "\n",
        "# PREDICT FUNCTION\n",
        "def predict_skills(text, model, tfidf, mlb, thresholds):\n",
        "    text_clean = clean_text(text)\n",
        "    X_new = tfidf.transform([text_clean])\n",
        "\n",
        "    proba = 1 / (1 + np.exp(-model.decision_function(X_new)))[0]\n",
        "    pred = (proba > thresholds).astype(int)\n",
        "\n",
        "    predicted_skills = mlb.inverse_transform(pred.reshape(1, -1))[0]\n",
        "    skill_proba_pairs = [(skill, proba[class_to_index[skill]]) for skill in predicted_skills]\n",
        "    skill_proba_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"Input:\", text)\n",
        "    for i, (skill, p) in enumerate(skill_proba_pairs, 1):\n",
        "        print(f\"   {i:2d}. {skill:40s} (prob: {p:.3f})\")\n",
        "\n",
        "    return list(predicted_skills)\n",
        "\n",
        "# EXAMPLE\n",
        "if __name__ == \"__main__\":\n",
        "    test_cases = [\"Belajar Fundamental Aplikasi Android\t\"]\n",
        "    for t in test_cases:\n",
        "        predict_skills(t, lr_model, tfidf, mlb, class_thresholds)"
      ],
      "metadata": {
        "id": "SbkqM8ei1nCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDIKSI SKILL BERDASARKAN TRACKING PROGRESS PENGGUNA\n",
        "def predict_user_progress(df_user, model, tfidf, mlb, thresholds):\n",
        "    results = []\n",
        "\n",
        "    # daftar course valid dari sheet Skill Course\n",
        "    valid_courses = set(df['course_name'].str.lower().str.strip())\n",
        "\n",
        "    for _, row in df_user.iterrows():\n",
        "        name = row.get(\"name\", \"\")\n",
        "        email = row.get(\"email\", \"\")\n",
        "\n",
        "        lp_raw = row.get(\"learning_path_id\", \"\")\n",
        "        course_raw = row.get(\"course_name\", \"\")\n",
        "\n",
        "        lp_clean = \"\" if pd.isna(lp_raw) else lp_raw\n",
        "        course_clean = \"\" if pd.isna(course_raw) else course_raw\n",
        "\n",
        "        lp = clean_text(lp_clean)\n",
        "        course = clean_text(course_clean)\n",
        "\n",
        "        # VALIDASI COURSE\n",
        "        # Jika course_name user tidak ada di data Skill Course kosongkan\n",
        "        if course.lower().strip() not in valid_courses:\n",
        "            predicted = []\n",
        "        else:\n",
        "            text = f\"{lp} {course}\".strip()\n",
        "            predicted = predict_skills(text, model, tfidf, mlb, thresholds)\n",
        "\n",
        "        results.append({\n",
        "            \"name\": name,\n",
        "            \"email\": email,\n",
        "            \"predicted_skills\": \", \".join(predicted)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# LOAD USER PROGRESS SHEET & RUN PREDICTION\n",
        "df_user = pd.read_excel(PATH, sheet_name=\"Tracking Progress Pengguna\")\n",
        "\n",
        "user_skill_predictions = predict_user_progress(df_user, lr_model, tfidf, mlb, class_thresholds)\n",
        "\n",
        "user_skill_predictions\n"
      ],
      "metadata": {
        "id": "aF0IrlKbTdex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apa yang sebaiknya dipelajari minggu ini?"
      ],
      "metadata": {
        "id": "Uio1pi_ZowD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "8N1R2dhxC8Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASS LEARNING TRACKER\n",
        "class SimpleLearningTracker:\n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def prepare_data(self, lp_answer, course, stud_progress):\n",
        "        course_data = course.copy()\n",
        "        stud_metrics = stud_progress.copy()\n",
        "\n",
        "        # Merge course info\n",
        "        stud_metrics = stud_metrics.merge(\n",
        "            course_data[['course_name', 'course_level_str', 'hours_to_study', 'learning_path_id']],\n",
        "            on='course_name',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Encode categorical\n",
        "        for col in ['course_level_str', 'course_name']:\n",
        "            if col in stud_metrics.columns:\n",
        "                le = LabelEncoder()\n",
        "                stud_metrics[col] = stud_metrics[col].astype(str)\n",
        "                stud_metrics[f'{col}_encoded'] = le.fit_transform(stud_metrics[col])\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        if 'password' not in stud_metrics.columns:\n",
        "            stud_metrics['password'] = 'N/A'\n",
        "\n",
        "        return stud_metrics, course_data\n",
        "\n",
        "    def recommend_courses(self, course_name, course_data, top_n=3):\n",
        "        current_course = course_data[course_data['course_name'] == course_name]\n",
        "        if current_course.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        current_level_str = str(current_course.iloc[0]['course_level_str']).strip()\n",
        "        current_lp_id = current_course.iloc[0]['learning_path_id']\n",
        "        try:\n",
        "            current_level_num = int(current_level_str)\n",
        "        except:\n",
        "            current_level_num = 0\n",
        "\n",
        "        filtered = course_data[course_data['learning_path_id'] == current_lp_id].copy()\n",
        "        filtered['course_level_int'] = pd.to_numeric(filtered['course_level_str'], errors='coerce').fillna(0).astype(int)\n",
        "        same_path_courses = filtered[\n",
        "            (filtered['course_level_int'] > current_level_num) &\n",
        "            (filtered['course_name'] != course_name)\n",
        "        ].sort_values('course_level_int')\n",
        "\n",
        "        level_map = {\"1\": \"Dasar\", \"2\": \"Pemula\", \"3\": \"Menengah\", \"4\": \"Mahir\", \"5\": \"Profesional\"}\n",
        "\n",
        "        recommendations = []\n",
        "        for _, row in same_path_courses.head(top_n).iterrows():\n",
        "            recommendations.append({\n",
        "                'name': row['course_name'],\n",
        "                'course_difficulty': level_map.get(str(row['course_level_str']).strip(), \"N/A\"),\n",
        "                'hours_to_study': row.get('hours_to_study', 'N/A')\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(recommendations)\n",
        "\n",
        "# MAIN FUNCTION\n",
        "def main(lp_answer, course, stud_progress):\n",
        "    tracker = SimpleLearningTracker()\n",
        "    stud_metrics, course_data = tracker.prepare_data(lp_answer, course, stud_progress)\n",
        "    return tracker, stud_metrics, course_data\n",
        "\n",
        "# WEEKLY STUDY PLAN FUNCTION\n",
        "def weekly_study_plan(tracker, stud_metrics, course_data, weeks=4):\n",
        "    for email in stud_metrics['email'].unique():\n",
        "        student = stud_metrics[stud_metrics['email'] == email].iloc[0]\n",
        "        current_course = student['course_name']\n",
        "        recommendations = tracker.recommend_courses(current_course, course_data, top_n=weeks)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ðŸ‘¤ Student: {student['name']} ({email})\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"\\nCurrent Course: {current_course}\")\n",
        "\n",
        "        if not recommendations.empty:\n",
        "            print(\"\\nWeekly Study Plan:\")\n",
        "            for week in range(1, weeks+1):\n",
        "                if week <= len(recommendations):\n",
        "                    course_idx = (week-1) % len(recommendations)\n",
        "                    course_rec = recommendations.iloc[course_idx]\n",
        "                    duration = f\" ({course_rec['hours_to_study']}h)\" if course_rec['hours_to_study'] != 'N/A' else \"\"\n",
        "                    print(f\"Minggu {week}: {course_rec['name']} ({course_rec['course_difficulty']}){duration}\")\n",
        "        else:\n",
        "            print(\"Tidak ada rekomendasi\")\n",
        "\n",
        "# RUNNING\n",
        "tracker, stud_metrics, course_data = main(lp_answer, course, stud_progress)\n",
        "weekly_study_plan(tracker, stud_metrics, course_data, weeks=4)"
      ],
      "metadata": {
        "id": "ta_COxHLyt_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracking progress pengguna (Dashboard)"
      ],
      "metadata": {
        "id": "Cc_sv1pYqia1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "ZOxe8NGCC_vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridLearningRecommender:\n",
        "    def __init__(self):\n",
        "        self.content_similarity = None\n",
        "        self.success_predictor = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    # DATA PREPARATION\n",
        "    def prepare_data(self, lp_answer, course, stud_progress, tutorials):\n",
        "        course_features = lp_answer.copy()\n",
        "        text_columns = ['technologies', 'course_type', 'course_difficulty', 'summary']\n",
        "\n",
        "        for col in text_columns:\n",
        "            if col in course_features.columns:\n",
        "                course_features[col] = course_features[col].fillna('').astype(str)\n",
        "\n",
        "        course_features['combined_features'] = (\n",
        "            course_features['technologies'] + ' ' +\n",
        "            course_features['course_type'] + ' ' +\n",
        "            course_features['course_difficulty'] + ' ' +\n",
        "            course_features['summary']\n",
        "        )\n",
        "\n",
        "        course_data = course.copy()\n",
        "\n",
        "        stud_metrics = stud_progress.copy()\n",
        "        numeric_cols = ['completed_tutorials', 'active_tutorials', 'exam_score', 'submission_rating']\n",
        "        for col in numeric_cols:\n",
        "            if col in stud_metrics.columns:\n",
        "                stud_metrics[col] = pd.to_numeric(stud_metrics[col], errors='coerce').fillna(0)\n",
        "\n",
        "        total_tutorials = stud_metrics['completed_tutorials'] + stud_metrics['active_tutorials']\n",
        "        stud_metrics['completion_rate'] = np.where(\n",
        "            total_tutorials > 0,\n",
        "            (stud_metrics['completed_tutorials'] / total_tutorials) * 100,\n",
        "            0\n",
        "        )\n",
        "\n",
        "        stud_metrics = stud_metrics.merge(\n",
        "            course_data[['course_name', 'course_level_str', 'hours_to_study', 'learning_path_id']],\n",
        "            on='course_name',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        if 'hours_to_study' not in stud_metrics.columns:\n",
        "            stud_metrics['hours_to_study'] = 10\n",
        "        else:\n",
        "            stud_metrics['hours_to_study'] = pd.to_numeric(stud_metrics['hours_to_study'], errors='coerce').fillna(10)\n",
        "\n",
        "        for col in ['course_level_str', 'course_name']:\n",
        "            if col in stud_metrics.columns:\n",
        "                le = LabelEncoder()\n",
        "                stud_metrics[col] = stud_metrics[col].astype(str)\n",
        "                stud_metrics[f'{col}_encoded'] = le.fit_transform(stud_metrics[col])\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        if 'password' not in stud_metrics.columns:\n",
        "            stud_metrics['password'] = 'N/A'\n",
        "\n",
        "        return course_features, stud_metrics, course_data\n",
        "\n",
        "    # CONTENT-BASED MODEL\n",
        "    def build_content_based_model(self, course_features):\n",
        "        tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "        tfidf_matrix = tfidf.fit_transform(course_features['combined_features'])\n",
        "        self.content_similarity = cosine_similarity(tfidf_matrix)\n",
        "        return self.content_similarity\n",
        "\n",
        "    # COURSE RECOMMENDATION\n",
        "    def recommend_courses(self, course_name, course_features, course_data, top_n=3):\n",
        "        current_course = course_data[course_data['course_name'] == course_name]\n",
        "        if current_course.empty:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        current_level_str = str(current_course.iloc[0]['course_level_str']).strip()\n",
        "        current_lp_id = current_course.iloc[0]['learning_path_id']\n",
        "\n",
        "        try:\n",
        "            current_level_num = int(current_level_str)\n",
        "        except:\n",
        "            current_level_num = 0\n",
        "\n",
        "        filtered = course_data[course_data['learning_path_id'] == current_lp_id].copy()\n",
        "\n",
        "        # Convert level to integer\n",
        "        filtered['course_level_int'] = pd.to_numeric(\n",
        "            filtered['course_level_str'], errors='coerce'\n",
        "        ).fillna(0).astype(int)\n",
        "\n",
        "        # Take only courses with higher level\n",
        "        same_path_courses = filtered[\n",
        "            (filtered['course_level_int'] > current_level_num) &\n",
        "            (filtered['course_name'] != course_name)\n",
        "        ].sort_values('course_level_int')\n",
        "\n",
        "        level_map = {\n",
        "            \"1\": \"Dasar\",\n",
        "            \"2\": \"Pemula\",\n",
        "            \"3\": \"Menengah\",\n",
        "            \"4\": \"Mahir\",\n",
        "            \"5\": \"Profesional\"\n",
        "        }\n",
        "\n",
        "        recommendations = []\n",
        "        for _, row in same_path_courses.head(top_n).iterrows():\n",
        "            recommendations.append({\n",
        "                'name': row['course_name'],\n",
        "                'course_difficulty': level_map.get(str(row['course_level_str']).strip(), \"N/A\"),\n",
        "                'technologies': row.get('technologies', 'N/A'),\n",
        "                'hours_to_study': row.get('hours_to_study', 'N/A')\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(recommendations)\n",
        "\n",
        "    # CLASSIFICATION MODEL\n",
        "    def build_classification_model(self, stud_metrics):\n",
        "        features = ['completion_rate', 'active_tutorials', 'completed_tutorials',\n",
        "                    'course_level_str_encoded', 'hours_to_study']\n",
        "\n",
        "        X = stud_metrics[features].fillna(0)\n",
        "\n",
        "        y = ((stud_metrics['is_graduated'] == 1) |\n",
        "             (stud_metrics['exam_score'] > 75)).astype(int)\n",
        "\n",
        "        if len(y.unique()) == 1:\n",
        "            normalized_cr = stud_metrics['completion_rate'] / 100\n",
        "            normalized_exam = stud_metrics['exam_score'] / 100\n",
        "            weighted = (normalized_cr * 0.5 + normalized_exam * 0.5)\n",
        "            y = (weighted >= 0.65).astype(int)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique()) > 1 else None\n",
        "        )\n",
        "\n",
        "        self.success_predictor = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.success_predictor.fit(X_train, y_train)\n",
        "\n",
        "        y_train_pred = self.success_predictor.predict(X_train)\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "        y_test_pred = self.success_predictor.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "        return self.success_predictor, train_accuracy, test_accuracy\n",
        "\n",
        "    # SUCCESS PROBABILITY\n",
        "    def predict_success(self, student_data):\n",
        "        features = ['completion_rate', 'active_tutorials', 'completed_tutorials',\n",
        "                    'course_level_str_encoded', 'hours_to_study']\n",
        "\n",
        "        X = student_data[features].fillna(0)\n",
        "        if len(X) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cr = student_data['completion_rate'].values[0]\n",
        "        exam = student_data['exam_score'].values[0] if 'exam_score' in student_data else 0\n",
        "        sr = student_data['submission_rating'].values[0] if 'submission_rating' in student_data else 0\n",
        "\n",
        "        norm_cr = cr / 100\n",
        "        norm_exam = exam / 100\n",
        "        norm_sr = sr / 5\n",
        "\n",
        "        final_prob = (norm_cr * 0.25) + (norm_exam * 0.35) + (norm_sr * 0.40)\n",
        "\n",
        "        return min(final_prob, 0.95)\n",
        "\n",
        "    # LEARNING STRATEGY\n",
        "    def generate_learning_strategy(self, student_email, stud_metrics, course_features, course_data):\n",
        "        student = stud_metrics[stud_metrics['email'] == student_email]\n",
        "        if len(student) == 0:\n",
        "            return {\"error\": \"Student not found\"}\n",
        "\n",
        "        student = student.iloc[0]\n",
        "        current_course = student['course_name']\n",
        "\n",
        "        recommendations = self.recommend_courses(current_course, course_features, course_data, top_n=3)\n",
        "        recommended_list = recommendations.to_dict('records') if not recommendations.empty else []\n",
        "\n",
        "        temp_student = stud_metrics[stud_metrics['email'] == student_email].copy()\n",
        "        for col in ['completion_rate', 'exam_score', 'submission_rating', 'active_tutorials', 'completed_tutorials', 'hours_to_study']:\n",
        "            if col not in temp_student.columns:\n",
        "                temp_student[col] = 0\n",
        "\n",
        "        success_prob = self.predict_success(temp_student)\n",
        "\n",
        "        adaptive_roadmap = self._generate_adaptive_roadmap(student, success_prob)\n",
        "\n",
        "        strategy = {\n",
        "            \"student_name\": student['name'],\n",
        "            \"email\": student_email,\n",
        "            \"password\": student.get('password', 'N/A'),\n",
        "            \"current_course\": current_course,\n",
        "            \"completion_rate\": student.get('completion_rate', 0),\n",
        "            \"exam_score\": student.get('exam_score', 0),\n",
        "            \"submission_rating\": student.get('submission_rating', 0),\n",
        "            \"success_probability\": success_prob,\n",
        "            \"recommended_courses\": recommended_list,\n",
        "            \"adaptive_roadmap\": adaptive_roadmap\n",
        "        }\n",
        "        return strategy\n",
        "\n",
        "    # ADAPTIVE ROADMAP\n",
        "    def _generate_adaptive_roadmap(self, student, success_prob):\n",
        "        cr = student['completion_rate']\n",
        "        es = student['exam_score']\n",
        "        sr = student['submission_rating']\n",
        "        hours_left = student.get('hours_to_study', 10)\n",
        "\n",
        "        remaining_completion = 100 - cr\n",
        "\n",
        "        if success_prob >= 0.70:\n",
        "            weeks_needed = max(1, int((remaining_completion / 100) * hours_left / 10))\n",
        "            status_label = \"Excellent - On Track\"\n",
        "        elif success_prob >= 0.55:\n",
        "            weeks_needed = max(2, int((remaining_completion / 100) * hours_left / 8))\n",
        "            status_label = \"Good - Minor Adjustments Needed\"\n",
        "        elif success_prob >= 0.40:\n",
        "            weeks_needed = max(3, int((remaining_completion / 100) * hours_left / 6))\n",
        "            status_label = \"Moderate - Needs Improvement\"\n",
        "        else:\n",
        "            weeks_needed = max(4, int((remaining_completion / 100) * hours_left / 4))\n",
        "            status_label = \"At Risk - Immediate Action Required\"\n",
        "\n",
        "        roadmap = {\n",
        "            \"current_status\": {\n",
        "                \"overall_status\": status_label,\n",
        "                \"progress\": f\"{cr:.0f}% selesai\",\n",
        "                \"exam_performance\": f\"Score: {es:.0f}/100\",\n",
        "                \"submission_rating\": f\"Rating: {sr:.1f}/5\"\n",
        "            },\n",
        "            \"next_steps\": [],\n",
        "            \"estimated_completion\": f\"{weeks_needed} minggu\"\n",
        "        }\n",
        "\n",
        "        if success_prob >= 0.70:\n",
        "            roadmap[\"next_steps\"] = [\n",
        "                \"Step 1: Selesaikan modul advanced & final project\",\n",
        "                \"Step 2: Ambil certification exam\",\n",
        "                \"Step 3: Build portfolio showcase project\",\n",
        "                \"Step 4: Siap lanjut ke level berikutnya\"\n",
        "            ]\n",
        "        elif success_prob >= 0.55:\n",
        "            roadmap[\"next_steps\"] = [\n",
        "                \"Step 1: Selesaikan modul yang tertinggal (fokus pada weak areas)\",\n",
        "                \"Step 2: Review & retake practice exam untuk score >80\",\n",
        "                \"Step 3: Tingkatkan submission quality (target rating 4+)\",\n",
        "                \"Step 4: Complete semua assignments sebelum deadline\"\n",
        "            ]\n",
        "        elif success_prob >= 0.40:\n",
        "            roadmap[\"next_steps\"] = [\n",
        "                \"Step 1: Fokus pada fundamental concepts (review basics)\",\n",
        "                \"Step 2: Join study group atau minta bantuan mentor\",\n",
        "                \"Step 3: Submit minimal 50% assignments untuk feedback\",\n",
        "                \"Step 4: Target 70%+ completion rate dalam 2 minggu\"\n",
        "            ]\n",
        "        else:\n",
        "            roadmap[\"next_steps\"] = [\n",
        "                \"Step 1: URGENT - Meet dengan academic advisor\",\n",
        "                \"Step 2: Join intensive tutoring sessions\",\n",
        "                \"Step 3: Create daily study schedule dengan mentor\",\n",
        "                \"Step 4: Consider course retake atau schedule adjustment\"\n",
        "            ]\n",
        "\n",
        "        insights = []\n",
        "        if sr < 2:\n",
        "            insights.append(\"Submission rating rendah - perlu improve assignment quality\")\n",
        "        if cr < 50:\n",
        "            insights.append(\"Completion rate <50% - accelerate learning pace\")\n",
        "        elif cr < 70:\n",
        "            insights.append(\"Completion rate moderate - maintain steady progress\")\n",
        "        if es < 70:\n",
        "            insights.append(\"Exam score <70 - review fundamental concepts\")\n",
        "        elif es >= 90:\n",
        "            insights.append(\"Excellent exam performance - strong understanding!\")\n",
        "        if sr == 0:\n",
        "            insights.append(\"Belum ada submission - mulai kerjakan assignments!\")\n",
        "\n",
        "        if es >= 90 and cr < 60:\n",
        "            insights.append(\"High exam score but low completion - focus on finishing modules!\")\n",
        "        if cr >= 80 and es < 60:\n",
        "            insights.append(\"High completion but low exam - review concepts more deeply!\")\n",
        "\n",
        "        roadmap[\"insights\"] = insights if insights else [\"Keep up the good work!\"]\n",
        "\n",
        "        return roadmap\n",
        "\n",
        "# MAIN FUNCTION\n",
        "def main(lp_answer, course, stud_progress, tutorials):\n",
        "    recommender = HybridLearningRecommender()\n",
        "    course_features, stud_metrics, course_data = recommender.prepare_data(lp_answer, course, stud_progress, tutorials)\n",
        "    recommender.build_content_based_model(course_features)\n",
        "    model, train_acc, test_acc = recommender.build_classification_model(stud_metrics)\n",
        "\n",
        "    return recommender, stud_metrics, course_features, course_data, train_acc, test_acc\n",
        "\n",
        "# RUNNING OUTPUT\n",
        "recommender, stud_metrics, course_features, course_data, train_acc, test_acc = main(\n",
        "    lp_answer, course, stud_progress, tutorials\n",
        ")\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
        "print(f\"Testing Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Nlv5LGku4Y6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT STUDENT RESULTS\n",
        "for email in stud_metrics['email'].unique():\n",
        "    strategy = recommender.generate_learning_strategy(\n",
        "        email, stud_metrics, course_features, course_data\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ‘¤ Student: {strategy['student_name']} ({strategy['email']})\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(f\"\\nCurrent Course:\")\n",
        "    print(f\"  - {strategy['current_course']}\")\n",
        "\n",
        "    print(f\"\\nCurrent Status:\")\n",
        "    print(f\"  - Completion Rate: {strategy['completion_rate']:.1f}%\")\n",
        "    print(f\"  - Exam Score: {strategy['exam_score']:.0f}/100\")\n",
        "    print(f\"  - Submission Rating: {strategy['submission_rating']:.1f}/5\")\n",
        "    print(f\"  - Success Probability: {strategy['success_probability']*100:.1f}%\")\n",
        "    print(f\"  - Overall Status: {strategy['adaptive_roadmap']['current_status']['overall_status']}\")\n",
        "\n",
        "    print(f\"\\nRecommended Next Courses:\")\n",
        "    if strategy['recommended_courses']:\n",
        "        for i, course_rec in enumerate(strategy['recommended_courses'], 1):\n",
        "            duration = f\" ({course_rec['hours_to_study']}h)\" if course_rec['hours_to_study'] != 'N/A' else \"\"\n",
        "            print(f\"  {i}. {course_rec['name']} ({course_rec['course_difficulty']}) - Duration{duration}\")\n",
        "    else:\n",
        "        print(\"Tidak ada rekomendasi\")\n",
        "\n",
        "    print(f\"\\nLearning Roadmap:\")\n",
        "    for i, step in enumerate(strategy['adaptive_roadmap']['next_steps'], 1):\n",
        "        print(f\"  {i}. {step}\")\n",
        "\n",
        "    print(f\"\\nEstimated Completion: {strategy['adaptive_roadmap']['estimated_completion']}\")\n",
        "\n",
        "    if strategy['adaptive_roadmap']['insights']:\n",
        "        print(f\"\\nKey Insights:\")\n",
        "        for insight in strategy['adaptive_roadmap']['insights']:\n",
        "            print(f\"  - {insight}\")"
      ],
      "metadata": {
        "id": "qGYqyFCB57Gd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}